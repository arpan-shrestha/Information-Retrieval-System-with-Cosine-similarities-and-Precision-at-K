{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jMpZErefvlkw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "leQRDEpQJKmf",
        "outputId": "2b9abba5-440b-4954-e734-d737e4ad1999",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "def term_frequency(term, document):\n",
        "    term_count = document.count(term)\n",
        "    total_terms = len(document)\n",
        "    return term_count / total_terms if total_terms > 0 else 0\n",
        "\n",
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for document in all_documents if term in document)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))\n",
        "\n",
        "def calculate_precision_at_k(relevant_docs, ranked_docs, k):\n",
        "    if not ranked_docs:\n",
        "        return 0.0\n",
        "    top_k_docs = ranked_docs[:k]\n",
        "    relevant_retrieved = sum(1 for doc_id, _ in top_k_docs if doc_id in relevant_docs)\n",
        "    return relevant_retrieved / k\n",
        "\n",
        "corpus_dir = '/content/docs'\n",
        "\n",
        "documents = []\n",
        "filenames = []\n",
        "for filename in os.listdir(corpus_dir):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        with open(os.path.join(corpus_dir, filename), 'r', encoding='utf-8') as file:\n",
        "            doc_text = file.read()\n",
        "            documents.append(preprocess(doc_text))\n",
        "            filenames.append(filename)\n",
        "\n",
        "relevance_dict = {\n",
        "    \"Tasmanian Aboriginal And PResident\": [\"doc2.txt\", \"doc4.txt\"],\n",
        "    \"Scientists typhoons and hurricanes Francis\": [\"doc10.txt\", \"doc5.txt\"],\n",
        "    \"Oceangate’s Titan and Space\": [\"doc5.txt\", \"doc9.txt\"],\n",
        "}\n",
        "\n",
        "queries = [\n",
        "    \"Tasmanian Aboriginal And PResident\",\n",
        "    \"Scientists typhoons and hurricanes Francis\",\n",
        "    \"Oceangate’s Titan and Space\",\n",
        "]\n",
        "\n",
        "K = 5\n",
        "\n",
        "with open(\"result.txt\", \"w\") as result_file:\n",
        "    for query in queries:\n",
        "        processed_query = preprocess(query)\n",
        "\n",
        "        all_terms = set([term for doc in documents for term in doc]).union(set(processed_query))\n",
        "\n",
        "        tfidf_documents = []\n",
        "        for doc in documents:\n",
        "            tfidf_vector = []\n",
        "            for term in all_terms:\n",
        "                tf = term_frequency(term, doc)\n",
        "                idf = inverse_document_frequency(term, documents)\n",
        "                tfidf_vector.append(tf * idf)\n",
        "            tfidf_documents.append(tfidf_vector)\n",
        "\n",
        "        tfidf_query = []\n",
        "        for term in all_terms:\n",
        "            tf = term_frequency(term, processed_query)\n",
        "            idf = inverse_document_frequency(term, documents)\n",
        "            tfidf_query.append(tf * idf)\n",
        "\n",
        "        tfidf_documents = np.array(tfidf_documents)\n",
        "        tfidf_query = np.array([tfidf_query])\n",
        "\n",
        "        cosine_similarities = cosine_similarity(tfidf_query, tfidf_documents).flatten()\n",
        "\n",
        "        ranked_results = sorted(zip(filenames, cosine_similarities), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        relevant_files = set(relevance_dict.get(query, []))\n",
        "        precision_at_k = calculate_precision_at_k(relevant_files, ranked_results, K)\n",
        "\n",
        "        result_file.write(f\"Cosine similarities for Query: '{query}'\\n\")\n",
        "        for i, (filename, score) in enumerate(ranked_results):\n",
        "            result_file.write(f\"Cosine similarity between Query and '{filename}': {score:.4f}\\n\")\n",
        "        result_file.write(f\"Precision@{K} for Query: '{query}': {precision_at_k:.4f}\\n\\n\")\n",
        "\n",
        "print(\"Cosine similarities and Precision at K written to result.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8tX9EZjAIAL",
        "outputId": "4117edf9-481a-4109-ce26-24d88639ee10"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarities and Precision at K written to result.txt\n"
          ]
        }
      ]
    }
  ]
}